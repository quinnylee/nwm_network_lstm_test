{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Union\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Empty cache before every run so that RAM doesn't blow up\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Parquet files\n",
    "DATA_DIR = Path(\"BIG_DATA/2008/\")\n",
    "print(sorted(os.listdir(DATA_DIR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(directory_path: Union[str, Path]) -> pd.DataFrame:\n",
    "    '''\n",
    "    Read data from a directory of Parquet files into a pandas DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory_path : str\n",
    "        Path to directory of Parquet files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame\n",
    "        Consolidated information from Parquet files.\n",
    "    '''\n",
    "    files = sorted(os.listdir(directory_path))\n",
    "    data = pd.read_parquet(directory_path + files[0])\n",
    "    data.drop(['LQFRAC'], axis = 1, inplace = True)\n",
    "    data.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "    for i in range(1,len(os.listdir(directory_path))):\n",
    "        data_intermediate = pd.read_parquet(directory_path + files[i])\n",
    "        data_intermediate.rename(columns={'Unnamed: 0': 'time'}, inplace=True)\n",
    "        data_intermediate.drop(['LQFRAC'], axis = 1, inplace = True)\n",
    "        data = pd.concat([data, data_intermediate], axis = 0)\n",
    "\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data: pd.DataFrame = collect_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "print(f\"The number of features available to us: {data.shape[1]}\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns\n",
    "data.drop(['bexp_soil_layers_stag=2',\n",
    "       'bexp_soil_layers_stag=3', 'bexp_soil_layers_stag=4', \n",
    "       'dksat_soil_layers_stag=2', 'dksat_soil_layers_stag=3', \n",
    "       'dksat_soil_layers_stag=4', 'psisat_soil_layers_stag=2',\n",
    "       'psisat_soil_layers_stag=3', 'psisat_soil_layers_stag=4', \n",
    "       'quartz_soil_layers_stag=2', 'quartz_soil_layers_stag=3', \n",
    "       'quartz_soil_layers_stag=4', 'smcmax_soil_layers_stag=2',\n",
    "       'smcmax_soil_layers_stag=3', 'smcmax_soil_layers_stag=4',\n",
    "       'smcwlt_soil_layers_stag=2', 'smcwlt_soil_layers_stag=3', \n",
    "       'smcwlt_soil_layers_stag=4',  'bexp_soil_layers_stag=1', 'cwpvt', 'mp', \n",
    "       'quartz_soil_layers_stag=1', 'vcmx25', 'aspect_c_mean', 'gw_Coeff', \n",
    "       'gw_Zmax', 'gw_Expon', 'psisat_soil_layers_stag=1', \n",
    "       'smcmax_soil_layers_stag=1', 'smcwlt_soil_layers_stag=1', 'refkdt'], \n",
    "       axis = 1, inplace = True)\n",
    "print(f\"After dropping a few columns, the number of new features available to \"\\\n",
    "      \"us is: {len(data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_in_dictionary(\n",
    "        data: pd.DataFrame\n",
    "        ) -> Tuple[Dict[int, pd.DataFrame], int, pd.DataFrame]:\n",
    "    '''\n",
    "    Load data into a dictionary. Each index of dictionary has a pair of \n",
    "    watersheds, upper and lower, and their features are merged in the for loop \n",
    "    below. Watersheds with invalid data from the input DataFrame are not \n",
    "    included in the dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        Consolidated model data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The keys contain the basin pair indices. The values are a DataFrame\n",
    "        that contain the information from the input DataFrame that correspond\n",
    "        specifically to a basin pair.\n",
    "\n",
    "    int\n",
    "        The number of basin pairs in the dictionary.\n",
    "\n",
    "    pd.DataFrame\n",
    "        One example DataFrame for a basin pair. This is used later to define\n",
    "        columns for our final dataset.\n",
    "    '''\n",
    "    num_networks = 0\n",
    "    network_dict = {}\n",
    "\n",
    "    for i in range(int((data['pair_id']).max())+1):\n",
    "        downstream = data[(data['pair_id']== i) & (data['du'] == 'd')]\n",
    "        upstream = data[(data['pair_id']== i) & (data['du'] == 'u')]\n",
    "\n",
    "        if downstream.empty or downstream.isnull().values.any():\n",
    "            print(i, \" DS is empty\")\n",
    "            continue\n",
    "        if upstream.empty or upstream.isnull().values.any():\n",
    "            print(i, \" US is empty or the upstream flow is constant\")\n",
    "            continue\n",
    "\n",
    "        network = downstream.merge(upstream, on=\"time\")\n",
    "        network_dict[num_networks] = network\n",
    "        num_networks += 1\n",
    "\n",
    "    real_id = 0\n",
    "    for key in network_dict.keys():\n",
    "        if key != real_id:\n",
    "            network_dict[real_id] = network_dict.pop(key)\n",
    "        real_id += 1\n",
    "\n",
    "    return network_dict, num_networks, network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_dict, num_networks, network = load_data_in_dictionary(\n",
    "    data)\n",
    "\n",
    "print(f\"The number of networks we have:\\t{len(network_dict)}\")\n",
    "print(f\"The number of data points in each network:\\t{network_dict[0].shape[0]}\")\n",
    "print(f\"The number of features in each network:\\t{network_dict[0].shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant information\n",
    "network.drop(['time', 'comid_x', 'catid_x', 'du_x', 'pair_id_x', 'twi_dist_4_x', \n",
    "              'comid_y', 'catid_y', 'du_y', 'pair_id_y', 'twi_dist_4_y', \"X_x\", \n",
    "              \"Y_x\", \"X_y\", \"Y_y\"], axis = 1, inplace=True)\n",
    "\n",
    "reduced_features = network.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA enabled devices\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using CUDA device:\", torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple M3/M2/M1 (Metal) device\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Define sequence length\n",
    "seq_length = 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate train-val-test split\n",
    "n_networks = len(network_dict)\n",
    "n_networks_split = {\"train\": (math.floor(n_networks * 0.7)), \n",
    "                    \"val\": math.floor(n_networks * 0.2), \n",
    "                    \"test\": math.floor(n_networks * 0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_parameters(\n",
    "        n_networks: int, \n",
    "        n_networks_split: Dict[str, int] = n_networks_split\n",
    "        ) -> List[List[List[int]]]:\n",
    "    ''' \n",
    "    Create list of network indices for each split.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_networks : int\n",
    "        Number of networks in dictionary.\n",
    "    n_networks_split : dict\n",
    "        Number of networks in train/validation/test splits.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list where each element is a sub-list containing the indices of the \n",
    "        networks in each split. The first element is the indices of the training\n",
    "        data, the second element is the indices of the validation data, and the \n",
    "        third element is the indices of the testing data.\n",
    "    '''\n",
    "    networks_for_training = list(range(0, n_networks_split['train'] + 1))\n",
    "    networks_for_val = list(range(n_networks_split['train'] + 1, \n",
    "                                 n_networks_split['train'] + n_networks_split[\n",
    "                                     'val'] + 1))\n",
    "\n",
    "    if (n_networks_split['test'] > 0):\n",
    "        networks_for_test = list(range(n_networks - n_networks_split['test'], \n",
    "                                       n_networks))\n",
    "    else:\n",
    "        print(\"Since the dataset is small and no basins are available for \"\\\n",
    "              \"testing. We give a random bucket to test.\")\n",
    "        random_bucket = np.random.randint(0, n_networks)\n",
    "        networks_for_test = [random_bucket]\n",
    "    \n",
    "    # organize the split parameters into separate lists for each set\n",
    "    train_split_parameters = [networks_for_training]\n",
    "    val_split_parameters = [networks_for_val]\n",
    "    test_split_parameters = [networks_for_test]\n",
    "\n",
    "    return [train_split_parameters, val_split_parameters, test_split_parameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[networks_for_training], \n",
    " [networks_for_val], \n",
    " [networks_for_test]] = split_parameters(n_networks)\n",
    "print(f\"The networks for training is {networks_for_training}\")\n",
    "print(f\"The networks for validation is {networks_for_val}\")\n",
    "print(f\"The networks for testing is {networks_for_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_with_lstm_output(\n",
    "        correlation_inputs: List[str],\n",
    "        correlation_outputs: List[str],\n",
    "        networks_for_training: List[int],\n",
    "        network_dict: Dict[int, pd.DataFrame] = network_dict\n",
    "    ) -> None:\n",
    "    '''\n",
    "    Generate a correlation matrix given model inputs. This function is \n",
    "    currently not used in our workflow. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    correlation_inputs : list\n",
    "        List of model inputs.\n",
    "    correlation_outputs : list\n",
    "        List of model outputs.\n",
    "    networks_for_training : list\n",
    "        List of indices of networks in training set\n",
    "    network_dict : dict\n",
    "        The keys contain the basin pair indices. The values are a DataFrame\n",
    "        that contain the information from the input DataFrame that correspond\n",
    "        specifically to a basin pair.\n",
    "    '''\n",
    "    correlation_data = correlation_inputs + correlation_outputs\n",
    "    frames = [network_dict[ibuc].loc[:, correlation_data] \n",
    "              for ibuc in networks_for_training]\n",
    "    df_in = pd.concat(frames)\n",
    "    correlation_matrix = np.corrcoef(df_in)\n",
    "\n",
    "    for i in range(len(correlation_outputs)):\n",
    "        print(f\"Correlation with {correlation_outputs[i]}\")\n",
    "        for j in range(len(correlation_data)):\n",
    "            print(f\"{correlation_data[j]}: \"/\n",
    "                  f\"{correlation_matrix[i+len(correlation_inputs)][j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_outputs = ['streamflow_y'] # what we're trying to predict using LSTM\n",
    "n_output = len(lstm_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_inputs = network.drop([\"streamflow_y\"], axis = 1).columns.to_list()\n",
    "n_input = len(lstm_inputs)\n",
    "print(f\"The number of features for LSTM model:\\t{n_input}\")\n",
    "print(lstm_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_networks(\n",
    "        ibuc: int, \n",
    "        network_dict: Dict[int, pd.DataFrame] = network_dict\n",
    "        )-> None:\n",
    "    '''  \n",
    "    Visualize upstream flow over time in each basin network over time.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ibuc : int\n",
    "        Index of network.\n",
    "    network_dict : dict\n",
    "        The keys contain the basin pair indices. The values are a DataFrame\n",
    "        that contain the information from the input DataFrame that correspond\n",
    "        specifically to a basin pair.\n",
    "    '''\n",
    "    fig, ax = plt.subplots()\n",
    "    print('Network:', ibuc)\n",
    "    print(\"Streamflow (downstream) mean:\", \n",
    "          np.round(network_dict[ibuc].streamflow_x.mean(), 2))\n",
    "    print(\"Streamflow (upstream) mean:\", \n",
    "          np.round(network_dict[ibuc].streamflow_y.mean(),2))\n",
    "\n",
    "    network_dict[ibuc].loc[:2000,['streamflow_y']].plot(ax=ax, legend=False)\n",
    "\n",
    "    ax.set_title('Streamflow')\n",
    "    ax.set_xlabel('Time (h)')\n",
    "    ax.set_ylabel('Streamflow')\n",
    "\n",
    "    ax.legend([\"Streamflow_y\"])\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def viz_networks2(\n",
    "        ibuc: int, \n",
    "        network_dict: Dict[int, pd.DataFrame] = network_dict\n",
    "        )-> None:\n",
    "    '''  \n",
    "    Visualize upstream and downstream flow and precipitation in each basin \n",
    "    network over time. This function is not currently used in our workflow.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ibuc : int\n",
    "        Index of network.\n",
    "    network_dict : dict\n",
    "        The keys contain the basin pair indices. The values are a DataFrame\n",
    "        that contain the information from the input DataFrame that correspond\n",
    "        specifically to a basin pair.\n",
    "    '''\n",
    "    fig, ax1 = plt.subplots()\n",
    "    print('Network:', ibuc)\n",
    "    print(\"Streamflow (downstream) mean:\", \n",
    "          np.round(network_dict[ibuc].streamflow_x.mean(), 2))\n",
    "    print(\"Streamflow (upstream) mean:\", \n",
    "          np.round(network_dict[ibuc].streamflow_y.mean(), 2))\n",
    "\n",
    "    # Plot streamflow_y on the primary y-axis\n",
    "    network_dict[ibuc].loc[:2000, ['streamflow_x']].plot(ax=ax1, \n",
    "                                                         color = 'blue', \n",
    "                                                         legend=False)\n",
    "    network_dict[ibuc].loc[:2000, ['streamflow_y']].plot(ax=ax1, \n",
    "                                                         color = 'purple', \n",
    "                                                         legend=False)\n",
    "    ax1.set_title('Streamflow and Precipitation')\n",
    "    ax1.set_xlabel('Time (h)')\n",
    "    ax1.set_ylabel('Streamflow')\n",
    "    \n",
    "    # Create a secondary y-axis and plot APCP_surface_x on it\n",
    "    ax2 = ax1.twinx()\n",
    "    network_dict[ibuc].loc[:2000, ['APCP_surface_x']].plot(ax=ax2, \n",
    "                                                           color='orange', \n",
    "                                                           legend=False)\n",
    "    \n",
    "    network_dict[ibuc].loc[:2000, ['APCP_surface_y']].plot(ax=ax2, \n",
    "                                                           color='red', \n",
    "                                                           legend=False)\n",
    "    ax2.set_ylabel('APCP Surface')\n",
    "    \n",
    "    # Add legends for each axis\n",
    "    ax1.legend([\"Streamflow_y\"], loc='upper left')\n",
    "    ax2.legend([\"APCP_surface_x\"], loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayed = 0\n",
    "\n",
    "for ibuc in networks_for_training:\n",
    "    if displayed < 20: # ensures outputs don't get spammed with figures\n",
    "        viz_networks(ibuc)\n",
    "        displayed += 1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    '''  \n",
    "    A class representing an LSTM model.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    input_size : int\n",
    "        Number of input features.\n",
    "    num_classes : int\n",
    "        Number of output features.\n",
    "    lstm1 : nn.LSTM\n",
    "        LSTM layer.\n",
    "    dropout1 : nn.Dropout\n",
    "        Dropout layer.\n",
    "    lstm2 : nn.LSTM\n",
    "        LSTM layer.\n",
    "    relu2 : nn.ReLU\n",
    "        Rectified linear unit (ReLU) layer. Non-linear activation function.\n",
    "    dropout2 : nn.Dropout\n",
    "        Dropout layer.\n",
    "    normalization1 : nn.BatchNorm1d\n",
    "        Normalization layer.\n",
    "    fc : nn.Sequential\n",
    "        A sequential container.\n",
    "    '''\n",
    "    def __init__(self, input_size: int, num_classes: int) -> None:\n",
    "        '''\n",
    "        Construct an LSTM1 object.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input features.\n",
    "        num_classes : int\n",
    "            Number of output features.\n",
    "        '''\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=input_size, \n",
    "                             hidden_size=128, \n",
    "                             batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.lstm2 = nn.LSTM(input_size=128, hidden_size=256, batch_first=True)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "        self.normalization1 = nn.BatchNorm1d(256)\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 128, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Linear(128, 64, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        '''  \n",
    "        Pass input data through the LSTM's layers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Model predictions based on input data.\n",
    "        '''\n",
    "        out, _= self.lstm1(x)\n",
    "        out = self.dropout1(out)\n",
    "        out, _ = self.lstm2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = out[:,-1, :]\n",
    "        out = self.normalization1(out)\n",
    "\n",
    "        prediction = self.fc(out)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM1(input_size=n_input, num_classes=n_output).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_scaler(\n",
    "        network_dict: Dict[int, pd.DataFrame] = network_dict,\n",
    "        networks_for_training: List[int] = networks_for_training,\n",
    "        lstm_inputs: List[str] = lstm_inputs,\n",
    "        lstm_outputs: List[str] = lstm_outputs\n",
    "    ) -> Tuple[StandardScaler, StandardScaler]:\n",
    "    '''  \n",
    "    Create scalers fit on training data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network_dict : dict\n",
    "        The keys contain the basin pair indices. The values are a DataFrame\n",
    "        that contain the information from the input DataFrame that correspond\n",
    "        specifically to a basin pair.\n",
    "    networks_for_training : list\n",
    "        List of network indices in training set. \n",
    "    lstm_inputs : list\n",
    "        List of features included in model inputs.\n",
    "    lstm_outputs : list\n",
    "        List of features included in model outputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    StandardScaler\n",
    "        Scaler for input variables.\n",
    "    StandardScaler\n",
    "        Scaler for output variables.\n",
    "    '''\n",
    "    frames = [network_dict[ibuc].loc[:, lstm_inputs] \n",
    "              for ibuc in networks_for_training]\n",
    "    df_in = pd.concat(frames)   \n",
    "    print(df_in.shape)\n",
    "    scaler_in = StandardScaler()\n",
    "    scaler_in.fit(df_in)\n",
    "\n",
    "    frames = [network_dict[ibuc].loc[:, lstm_outputs] \n",
    "              for ibuc in networks_for_training]\n",
    "    df_out = pd.concat(frames)    \n",
    "    print(df_out.shape)\n",
    "    scaler_out = StandardScaler()\n",
    "    scaler_out.fit(df_out)\n",
    "    return scaler_in, scaler_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_in, scaler_out = fit_scaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Dataset class that loads data only when necessary to preserve memory.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    network_dict : dict\n",
    "        The keys contain the basin pair indices. The values are a DataFrame\n",
    "        that contain the information from the input DataFrame that correspond\n",
    "        specifically to a basin pair.\n",
    "    network_list : list\n",
    "        List of network indices for a particular train/validation/test split.\n",
    "    scaler_in : StandardScaler\n",
    "        Scaler fit on input data.\n",
    "    scaler_out : StandardScaler\n",
    "        Scaler fit on output data.\n",
    "    df : pd.DataFrame\n",
    "        Concatenated data from network_dict\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            network_dict: Dict[int, pd.DataFrame] = network_dict,\n",
    "            network_list: List[int] = networks_for_training,\n",
    "            scaler_in: StandardScaler = scaler_in,\n",
    "            scaler_out: StandardScaler = scaler_out\n",
    "        ) -> None:\n",
    "        ''' \n",
    "        Construct an EfficientDataset class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network_dict : dict\n",
    "            The keys contain the basin pair indices. The values are a DataFrame\n",
    "            that contain the information from the input DataFrame that \n",
    "            correspond specifically to a basin pair.\n",
    "        network_list : list\n",
    "            List of network indices for a particular train/validation/test \n",
    "            split.\n",
    "        scaler_in : StandardScaler\n",
    "            Scaler fit on input data.\n",
    "        scaler_out : StandardScaler\n",
    "            Scaler fit on output data.\n",
    "        '''\n",
    "        self.network_dict = network_dict\n",
    "        self.network_list = network_list\n",
    "        self.scaler_in = scaler_in\n",
    "        self.scaler_out = scaler_out\n",
    "        self.df = pd.concat([network_dict[key] for key in network_list],\n",
    "                            ignore_index=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        '''Return number of networks in network_list.'''\n",
    "        return len(self.df) \n",
    "    \n",
    "    def transform(self, object: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        '''  \n",
    "        Transform untransformed data using scalers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        object : pd.DataFrame\n",
    "            Untransformed information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Transformed input data.\n",
    "        np.ndarray\n",
    "            Transformed output data.\n",
    "        '''\n",
    "        object_in = object.loc[:, lstm_inputs]\n",
    "        object_out = object.loc[:, lstm_outputs]\n",
    "        scaler_in_i = self.scaler_in.transform(object_in)\n",
    "        scaler_out_i = self.scaler_out.transform(object_out)\n",
    "\n",
    "        return scaler_in_i, scaler_out_i\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        '''\n",
    "        Transform and access one row (represents one timestep in one particular\n",
    "        basin network) in df.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            Index of row in df.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Transformed input data.\n",
    "        np.ndarray\n",
    "            Transformed output data.\n",
    "        '''\n",
    "        try:\n",
    "            object = self.df.iloc[[idx]]\n",
    "            scaler_in_i, scaler_out_i = self.transform(object)\n",
    "\n",
    "            return scaler_in_i.astype(np.float32), scaler_out_i.astype(\n",
    "                np.float32)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            df = self.network_dict[0]\n",
    "            print(\"number of time steps in one network\")\n",
    "            print(len(df))\n",
    "            print(\"total number of iterations in dataset\")\n",
    "            print(self.__len__())\n",
    "            print(\"number of networks in this list\")\n",
    "            print(len(self.network_list))\n",
    "            print(\"index at which fn failed\")\n",
    "            print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = EfficientDataset(network_dict, networks_for_training, scaler_in, \n",
    "                              scaler_out)\n",
    "val_data = EfficientDataset(network_dict, networks_for_val, scaler_in, \n",
    "                            scaler_out)\n",
    "test_data = EfficientDataset(network_dict, networks_for_test, scaler_in,\n",
    "                             scaler_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024 # Number of rows (timesteps) to load at a time\n",
    "shuffle = False # Determines if batches get shuffed when they are loaded\n",
    "num_workers = 15 # Number of workers used to load data\n",
    "\n",
    "# Create DataLoaders to iterate through EfficientDatasets\n",
    "train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=shuffle, \n",
    "                                           num_workers=num_workers)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=shuffle, \n",
    "                                         num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=shuffle, \n",
    "                                          num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_scale(\n",
    "        scaler: StandardScaler, \n",
    "        numpy_value: np.float32\n",
    ") -> np.float32:\n",
    "    '''\n",
    "    Invert scaler transformation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    scaler : StandardScaler\n",
    "        Scaler whose transformation is to be undone.\n",
    "    numpy_value : np.float32\n",
    "        Transformed value.\n",
    "\n",
    "    Returns \n",
    "    -------\n",
    "    np.float32\n",
    "        Untransformed value.\n",
    "    '''\n",
    "    transformed_numpy_value = scaler.inverse_transform(numpy_value)\n",
    "    return transformed_numpy_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_NSE(\n",
    "        observed_value: np.float32, \n",
    "        modeled_value: np.float32\n",
    "    ) -> np.float32:\n",
    "    '''  \n",
    "    Calculate Nash-Sutcliffe Efficiency (NSE) of LSTM prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observed_value : np.float32\n",
    "        Observed value.\n",
    "    modeled_value : np.float32\n",
    "        Predicted value.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.float32\n",
    "        NSE value.\n",
    "    '''\n",
    "    mean_observed_value = np.mean(observed_value)\n",
    "    numerator = np.sum(np.power((observed_value - modeled_value), 2))\n",
    "    denominator = np.sum(np.power((observed_value - mean_observed_value), 2))\n",
    "    \n",
    "    return round((1 - (numerator/denominator)), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    '''  \n",
    "    A class that monitors a model's performance on a validation set and stops\n",
    "    the training process when the loss does not improve to avoid overfitting.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of evaluations with no improvement before training stops.\n",
    "    min_delta : int\n",
    "        Minimum change in the loss to qualify as an improvement, i.e. a change\n",
    "        of less than min_delta will count as no improvement.\n",
    "    counter : int\n",
    "        Keeps count of number of evaluations with no improvement.\n",
    "    min_validation_loss : float\n",
    "        The loss value that future evaluations' losses need to be less than by\n",
    "        min_delta to count as an improvement.\n",
    "    '''\n",
    "    def __init__(self, patience: int = 5, min_delta: int = 0) -> None:\n",
    "        '''\n",
    "        Construct EarlyStopper class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        patience : int\n",
    "            Number of evaluations with no improvement before training stops.\n",
    "        min_delta : int\n",
    "            Minimum change in the loss to qualify as an improvement, i.e. a \n",
    "            change of less than min_delta will count as no improvement.  \n",
    "        '''\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss: float) -> bool:\n",
    "        '''  \n",
    "        Determine if training should stop. Training stops when counter is equal\n",
    "        to patience, i.e. the number of evaluations in a row where there was no\n",
    "        improvement in loss is equal to the patience value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        validation_loss : float\n",
    "            Most recent loss value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Training continues if False. Training stops if True.\n",
    "        '''\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "class EarlyStopperNSE:\n",
    "    '''  \n",
    "    A class that monitors a model's performance on a validation set and stops\n",
    "    the training process when the NSE does not improve to avoid overfitting.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    patience : int\n",
    "        Number of evaluations with no improvement before training stops.\n",
    "    min_delta : int\n",
    "        Minimum change in the NSE to qualify as an improvement, i.e. a change\n",
    "        of less than min_delta will count as no improvement.\n",
    "    counter : int\n",
    "        Keeps count of number of evaluations with no improvement.\n",
    "    max_NSE : float\n",
    "        The NSE value that future evaluations' NSEs need to be greater than by\n",
    "        min_delta to count as an improvement.\n",
    "    '''\n",
    "    def __init__(self, patience: int = 5, min_delta: int = 0) -> None:\n",
    "        '''\n",
    "        Construct EarlyStopperNSE class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        patience : int\n",
    "            Number of evaluations with no improvement before training stops.\n",
    "        min_delta : int\n",
    "            Minimum change in the NSE to qualify as an improvement, i.e. a \n",
    "            change of less than min_delta will count as no improvement.  \n",
    "        '''\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.max_NSE = -100000000\n",
    "    \n",
    "    def early_stop(self, NSE: float) -> bool:\n",
    "        '''  \n",
    "        Determine if training should stop. Training stops when counter is equal\n",
    "        to patience, i.e. the number of evaluations in a row where there was no\n",
    "        improvement in NSE is equal to the patience value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        NSE : float\n",
    "            Most recent NSE value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            Training continues if False. Training stops if True.\n",
    "        '''\n",
    "        if (NSE >= self.max_NSE):\n",
    "            self.max_NSE = NSE\n",
    "            self.counter = 0\n",
    "        elif NSE <= (self.max_NSE - self.min_delta):\n",
    "            self.counter += 1\n",
    "            if(self.counter) >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopper = EarlyStopper(patience = 15, min_delta= 0.001)\n",
    "early_stopper_NSE = EarlyStopperNSE(patience = 15, min_delta = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        lstm: LSTM1, \n",
    "        train_loader: torch.utils.data.DataLoader\n",
    "        ) -> Tuple[LSTM1, List, List, List, List]:\n",
    "    '''\n",
    "    Train LSTM1 model using mean squared error (MSE) as loss and AdamW as \n",
    "    optimizer.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lstm : LSTM1\n",
    "        An LSTM1 class.\n",
    "    train_loader : torch.utils.data.DataLoader\n",
    "        DataLoader generated for training data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    LSTM1\n",
    "        Trained LSTM1 model.\n",
    "    list\n",
    "        List of training MSEs.\n",
    "    list\n",
    "        List of validation MSEs.\n",
    "    list\n",
    "        List of training NSEs.\n",
    "    list\n",
    "        List of validation MSEs.\n",
    "    '''\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(lstm.parameters(), lr= 0.001, weight_decay = 0.09)\n",
    "    # reduces learning rate when loss has stopped improving\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                           patience = 10, \n",
    "                                                           factor = 0.8)\n",
    "    epochs = 2\n",
    "    loss_history_train = []\n",
    "    loss_history_val = []\n",
    "    NSE_train = []\n",
    "    NSE_val = []\n",
    "    for epoch in range(epochs):\n",
    "        lstm.train()\n",
    "        NSE_batch_train = []\n",
    "        NSE_batch_val = []\n",
    "        with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "            for data, targets in train_loader:\n",
    "                tepoch.set_description(f\"Epoch {epoch} Training\")\n",
    "\n",
    "                loss_history_train_epoch = []\n",
    "                loss_history_val_epoch = []\n",
    "                x = data.to(device=device)\n",
    "                y = targets.to(device=device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                predicted_y = lstm(x)\n",
    "                loss = criterion(predicted_y, y)\n",
    "\n",
    "                loss_history_train_epoch.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    y_calc =  y.to(\"cpu\").numpy()\n",
    "                    y_calc = y_calc.reshape(1, -1)\n",
    "                    y_calc = inverse_scale(scaler_out, y_calc)\n",
    "                    \n",
    "                    predicted_y_calc =  predicted_y.to(\"cpu\").numpy()    \n",
    "                    predicted_y_calc = predicted_y_calc.reshape(1, -1)  \n",
    "                    predicted_y_calc = inverse_scale(scaler_out, \n",
    "                                                     predicted_y_calc)\n",
    "                    \n",
    "                    NSE_batch_train.append(calculate_NSE(y_calc, \n",
    "                                                         predicted_y_calc))\n",
    "\n",
    "                tepoch.update(1)\n",
    "                tepoch.set_postfix(loss=loss.item(), \n",
    "                                   accuracy=NSE_batch_train[-1])\n",
    "                \n",
    "        lstm.eval()\n",
    "        with torch.no_grad():\n",
    "            with tqdm(val_loader, unit=\"batch\") as vepoch:\n",
    "                for data, targets in val_loader:\n",
    "                    vepoch.set_description(f\"Epoch {epoch} Validation\")\n",
    "\n",
    "                    x_calc = data.to(device=device)\n",
    "                    y_calc = targets.to(device=device)\n",
    "                    predicted_y_calc = lstm(x_calc)\n",
    "                    loss_calc = criterion(predicted_y_calc, y_calc)\n",
    "                    loss_history_val_epoch.append(loss_calc.item())\n",
    "\n",
    "                    y_calc =  y_calc.to(\"cpu\").numpy()\n",
    "                    y_calc = y_calc.reshape(1, -1)\n",
    "                    y_calc = inverse_scale(scaler_out, y_calc)\n",
    "                    \n",
    "                    predicted_y_calc =  predicted_y_calc.to(\"cpu\").numpy()\n",
    "                    predicted_y_calc = predicted_y_calc.reshape(1, -1)\n",
    "                    predicted_y_calc = inverse_scale(scaler_out, \n",
    "                                                     predicted_y_calc)\n",
    "\n",
    "                    NSE_batch_val.append(calculate_NSE(y_calc, \n",
    "                                                       predicted_y_calc))\n",
    "                    \n",
    "                    vepoch.update(1)\n",
    "                    vepoch.set_postfix(loss=loss.item(),\n",
    "                                       accuracy=NSE_batch_val[-1])\n",
    "\n",
    "        scheduler.step(loss_calc)\n",
    "        loss_history_train.append(np.mean(loss_history_train_epoch))\n",
    "        loss_history_val.append(np.mean(loss_history_val_epoch))\n",
    "        NSE_val.append(np.mean(NSE_batch_val))\n",
    "        NSE_train.append(np.mean(NSE_batch_train))\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1} Completed\")\n",
    "        print(f\"Loss_Train: {loss_history_train[-1]:.4f}\")\n",
    "        print(f\"NSE_Train: {np.mean(NSE_batch_train):.4f}\")\n",
    "        print(f\"Loss_Val: {loss_history_val[-1]:.4f}\")\n",
    "        print(f\"NSE_Validation: {np.mean(NSE_batch_val):.4f}\")\n",
    "        print(f\"Learning_Rate: {optimizer.param_groups[0]['lr']})\")       \n",
    "\n",
    "    return lstm, loss_history_train, loss_history_val, NSE_train, NSE_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(model, \n",
    " loss_history_train, \n",
    " loss_history_val, \n",
    " NSE_train, \n",
    " NSE_val) = train_model(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot change in loss over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history_train, label='Train', color='blue')\n",
    "plt.plot(loss_history_val, label='Validation', color='orange', linestyle = '--')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using test data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with tqdm(test_loader, unit=\"batch\") as testbar:\n",
    "        for data, targets in test_loader:\n",
    "            testbar.set_description(\"Testing!\")\n",
    "            \n",
    "            test_data_x = data.to(device=device)\n",
    "            test_data_y = targets.to(device=device)\n",
    "\n",
    "            predicted_y_untransformed = model(test_data_x)\n",
    "            predicted_y_untransformed = predicted_y_untransformed.reshape(1, -1)\n",
    "            predicted_y_untransformed = predicted_y_untransformed.to(\"cpu\")\n",
    "            predicted_y_untransformed = predicted_y_untransformed.numpy()\n",
    "            predicted_y = inverse_scale(scaler_out, predicted_y_untransformed)\n",
    "\n",
    "            test_data_y = test_data_y.reshape(1, -1)\n",
    "            test_data_y = test_data_y.to(\"cpu\").numpy()\n",
    "            test_data_y = inverse_scale(scaler_out, test_data_y)\n",
    "\n",
    "            testbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test data NSE\n",
    "print(calculate_NSE(test_data_y, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual and predicted streamflow values in test data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(test_data_y[0], label='Actual Streamflow', color='blue')\n",
    "plt.plot(predicted_y[0], label='Predicted Streamflow', color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Streamflow')\n",
    "plt.title('Actual vs Predicted Streamflow (Combined Model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
