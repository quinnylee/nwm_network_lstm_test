{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from typing import Tuple\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from exactextract import exact_extract\n",
    "from exactextract.raster import NumPyRasterSource\n",
    "from pathlib import Path\n",
    "from rich.progress import (\n",
    "    Progress,\n",
    "    BarColumn,\n",
    "    TextColumn,\n",
    "    TimeElapsedColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "import psutil\n",
    "from math import ceil\n",
    "from multiprocessing import shared_memory\n",
    "from functools import partial\n",
    "import warnings\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import shutil\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    message=\"'DataFrame.swapaxes' is deprecated\", \n",
    "    category=FutureWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    message=\"'GeoDataFrame.swapaxes' is deprecated\", \n",
    "    category=FutureWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time_range(dataset: xr.Dataset, \n",
    "                        start_time: str, \n",
    "                        end_time: str) -> Tuple[str, str]:\n",
    "    '''\n",
    "    Ensure that all selected times are in the passed dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Dataset with a time coordinate.\n",
    "    start_time : str\n",
    "        Desired start time in YYYY/MM/DD HH:MM:SS format.\n",
    "    end_time : str\n",
    "        Desired end time in YYYY/MM/DD HH:MM:SS format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        start_time, or if not available, earliest available timestep in dataset.\n",
    "    str\n",
    "        end_time, or if not available, latest available timestep in dataset.\n",
    "    '''\n",
    "    end_time_in_dataset = dataset.time.isel(time=-1).values\n",
    "    start_time_in_dataset = dataset.time.isel(time=0).values\n",
    "    if np.datetime64(start_time) < start_time_in_dataset:\n",
    "        logger.warning(\n",
    "            f\"provided start {start_time} is before the start of the dataset \"\\\n",
    "                \"{start_time_in_dataset}, selecting from \"\\\n",
    "                \"{start_time_in_dataset}\"\n",
    "        )\n",
    "        start_time = start_time_in_dataset\n",
    "    if np.datetime64(end_time) > end_time_in_dataset:\n",
    "        logger.warning(\n",
    "            f\"provided end {end_time} is after the end of the dataset \"\\\n",
    "                \"{end_time_in_dataset}, selecting until {end_time_in_dataset}\"\n",
    "        )\n",
    "        end_time = end_time_in_dataset\n",
    "    return start_time, end_time\n",
    "\n",
    "def clip_dataset_to_bounds(dataset: xr.Dataset, \n",
    "                           bounds: Tuple[float, float, float, float], \n",
    "                           start_time: str, \n",
    "                           end_time: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Clip the dataset to specified geographical bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Dataset to be clipped.\n",
    "    bounds : tuple[float, float, float, float]\n",
    "        Corners of bounding box. bounds[0] is x_min, bounds[1] is y_min, \n",
    "        bounds[2] is x_max, bounds[3] is y_max.\n",
    "    start_time : str\n",
    "        Desired start time in YYYY/MM/DD HH:MM:SS format.\n",
    "    end_time : str\n",
    "        Desired end time in YYYY/MM/DD HH:MM:SS format.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Clipped dataset.\n",
    "    \"\"\"\n",
    "    # check time range here in case just this function is imported and not the \n",
    "    # whole module\n",
    "    start_time, end_time = validate_time_range(dataset, start_time, end_time)\n",
    "    dataset = dataset.sel(\n",
    "        x=slice(bounds[0], bounds[2]),\n",
    "        y=slice(bounds[1], bounds[3]),\n",
    "        time=slice(start_time, end_time),\n",
    "    )\n",
    "    logger.info(\"Selected time range and clipped to bounds\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_gdf_selection(headwater: str, \n",
    "                       gdb: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    '''  \n",
    "    Select headwater row from GeoDataFrame containing all basins in study area.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    headwater : str\n",
    "        NWM 3.0 reach ID of headwater basin\n",
    "    gdb : gpd.GeoDataFrame\n",
    "        GeoDataFrame that contains geometry information about all basins in\n",
    "        study area.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    head_gdf : gpd.GeoDataFrame\n",
    "        The row in gdb that corresponds to the headwater basin.\n",
    "    '''\n",
    "    head_gdf = gdb.loc[gdb['ID'] == int(headwater)]\n",
    "    return head_gdf\n",
    "\n",
    "def tail_gdf_selection(headwater: str, \n",
    "                       tailwater: str, \n",
    "                       gdb: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    '''  \n",
    "    Select tailwater row from GeoDataFrame containing all basins in study area.\n",
    "    The returned geometry is the union of the polygons for the headwater basin\n",
    "    and the tailwater basin.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    headwater : str\n",
    "        NWM 3.0 reach ID of headwater basin\n",
    "    tailwater : str\n",
    "        NWM 3.0 reach ID of tailwater basin\n",
    "    gdb : gpd.GeoDataFrame\n",
    "        GeoDataFrame that contains geometry information about all basins in\n",
    "        study area.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tail_gdf : gpd.GeoDataFrame\n",
    "        The row in gdb that corresponds to the tailwater basin with a merged\n",
    "        geometry that encompasses both the headwater and tailwater polygons.\n",
    "    '''\n",
    "    tail_geom = gpd.GeoSeries(\n",
    "        [gdb.loc[int(headwater)]['geometry'],\n",
    "        gdb.loc[int(tailwater)]['geometry']]\n",
    "    ).union_all\n",
    "\n",
    "    d = gdb.loc[gdb['ID'] == int(tailwater)]\n",
    "    d['geometry'] = tail_geom[0]\n",
    "    tail_gdf = gpd.GeoDataFrame(d)\n",
    "\n",
    "    return tail_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_terminals(olddict: dict) -> dict:\n",
    "    '''\n",
    "    Remove entries in olddict whose values are negative numbers, aka the \n",
    "    tailwater basin is a terminal basin (e.g. ocean, lake, reservoir).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    olddict : dict\n",
    "        Dictionary of basin pairs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    newdict : dict\n",
    "        Dictionary of basin pairs that contains no terminal basins as tailwaters\n",
    "    '''\n",
    "    newdict = {k: v for k, v in list(olddict.items()) if v > 0} \n",
    "    return newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_weights(raster: xr.Dataset, \n",
    "                     gdf: gpd.GeoDataFrame, \n",
    "                     wkt: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Get the cell weights (coverage) for each cell in a divide. Coverage is \n",
    "    defined as the fraction (a float in [0,1]) of a raster cell that overlaps \n",
    "    with the polygon in the passed gdf.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raster : xr.Dataset\n",
    "        One timestep of a gridded forcings dataset.\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        A GeoDataFrame with a polygon feature.\n",
    "    wkt : str\n",
    "        Well-known text (WKT) representation of gdf's coordinate reference\n",
    "        system (CRS)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame indexed by divide_id that contains information about coverage\n",
    "        for each raster cell in gridded forcing file.\n",
    "    '''\n",
    "    xmin = raster.x[0]\n",
    "    xmax = raster.x[-1]\n",
    "    ymin = raster.y[0]\n",
    "    ymax = raster.y[-1]\n",
    "    rastersource = NumPyRasterSource(\n",
    "        raster[\"RAINRATE\"], \n",
    "        srs_wkt=wkt, \n",
    "        xmin=xmin, \n",
    "        xmax=xmax, \n",
    "        ymin=ymin, \n",
    "        ymax=ymax\n",
    "    )\n",
    "    output = exact_extract(\n",
    "        rastersource,\n",
    "        gdf,\n",
    "        [\"cell_id\", \"coverage\"],\n",
    "        include_cols=[\"divide_id\"],\n",
    "        output=\"pandas\",\n",
    "    )\n",
    "    return output.set_index(\"divide_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_weights_parallel(gdf: gpd.GeoDataFrame,\n",
    "                              input_forcings: xr.Dataset,\n",
    "                              num_partitions: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    Execute get_cell_weights with multiprocessing, with chunking for the passed\n",
    "    GeoDataFrame to conserve memory usage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        A GeoDataFrame with a polygon feature.\n",
    "    input_forcings : xr.Dataset\n",
    "        A gridded forcings file.\n",
    "    num_partitions : int\n",
    "        Number of chunks to split gdf into.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame indexed by divide_id that contains information about coverage\n",
    "        for each raster cell and each timestep in gridded forcing file.\n",
    "    '''\n",
    "    gdf_chunks = np.array_split(gdf, num_partitions)\n",
    "    wkt = gdf.crs.to_wkt()\n",
    "    one_timestep = input_forcings.isel(time=0).compute()\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        args = [(one_timestep, gdf_chunk, wkt) for gdf_chunk in gdf_chunks]\n",
    "        catchments = pool.starmap(get_cell_weights, args)\n",
    "    return pd.concat(catchments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_units(dataset: xr.Dataset) -> dict:\n",
    "    '''\n",
    "    Return dictionary of units for each variable in dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Dataset with variables and units.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict \n",
    "        Each key is a variable in dataset, and its value is the unit associated\n",
    "        with the variable.\n",
    "    '''\n",
    "    units = {}\n",
    "    for var in dataset.data_vars:\n",
    "        if dataset[var].attrs[\"units\"]:\n",
    "            units[var] = dataset[var].attrs[\"units\"]\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_chunks(data: xr.DataArray) -> list[tuple[int, int]]:\n",
    "    '''  \n",
    "    Take a DataArray and calculate the start and end index for each chunk based\n",
    "    on the available memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xr.DataArray\n",
    "        Large DataArray that can't be loaded into memory all at once.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Tuple[int, int]]\n",
    "        Each element in the list represents a chunk of data. The tuple within\n",
    "        the chunk indicates the start index and end index of the chunk.\n",
    "    '''\n",
    "    array_memory_usage = data.nbytes\n",
    "    free_memory = psutil.virtual_memory().available * 0.8  # 80% of available \n",
    "                                                            # memory\n",
    "    # limit the chunk to 20gb, makes things more stable\n",
    "    free_memory = min(free_memory, 20 * 1024 * 1024 * 1024)\n",
    "    num_chunks = ceil(array_memory_usage / free_memory)\n",
    "    max_index = data.shape[0]\n",
    "    stride = max_index // num_chunks\n",
    "    chunk_start = range(0, max_index, stride)\n",
    "    index_chunks = [(start, start + stride) for start in chunk_start]\n",
    "    return index_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_memory(lazy_array: xr.Dataset) -> Tuple[\n",
    "    shared_memory.SharedMemory,\n",
    "    np.dtype.shape,\n",
    "    np.dtype\n",
    "]:\n",
    "    '''\n",
    "    Create a shared memory object so that multiple processes can access loaded \n",
    "    data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lazy_array : xr.Dataset\n",
    "        A chunk of gridded forcing variable data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    shared_memory.SharedMemory\n",
    "        A specific block of memory allocated by the OS of the size of \n",
    "        lazy_array.\n",
    "    np.dtype.shape\n",
    "        A shape object with dimensions (# timesteps, # of raster cells) in\n",
    "        reference to lazy_array.\n",
    "    np.dtype\n",
    "        Data type of objects in lazy_array.\n",
    "    '''\n",
    "    logger.debug(f\"Creating shared memory size {lazy_array.nbytes/ 10**6} Mb.\")\n",
    "    shm = shared_memory.SharedMemory(create=True, size=lazy_array.nbytes)\n",
    "    shared_array = np.ndarray(lazy_array.shape, \n",
    "                              dtype=np.float32, \n",
    "                              buffer=shm.buf)\n",
    "    # if your data is not float32, xarray will do an automatic conversion here\n",
    "    # which consumes a lot more memory, forcings downloaded with this tool will \n",
    "    # work\n",
    "    for start, end in get_index_chunks(lazy_array):\n",
    "        # copy data from lazy to shared memory one chunk at a time\n",
    "        shared_array[start:end] = lazy_array[start:end]\n",
    "\n",
    "    time, x, y = shared_array.shape\n",
    "    shared_array = shared_array.reshape(time, -1)\n",
    "\n",
    "    return shm, shared_array.shape, shared_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_of_cells(flat_raster: np.ndarray, \n",
    "                          cell_ids: np.ndarray, \n",
    "                          factors: np.ndarray) -> np.ndarray:\n",
    "    '''  \n",
    "    Take an average of each forcing variable in a catchment. Create an output\n",
    "    array initialized with zeros. Sum up the forcing variable and divide by the \n",
    "    sum of the cell weights to get an averaged forcing variable for the entire \n",
    "    catchment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flat_raster : np.ndarray\n",
    "        An array of dimensions (time, x*y) containing forcing variable values\n",
    "        in each cell. Each element in the array corresponds to a cell ID.\n",
    "    cell_ids : np.ndarray\n",
    "        A list of the raster cell IDs that intersect the study catchment.\n",
    "    factors : np.ndarray\n",
    "        A list of the weights (coverages) of each cell in cell_ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An one-dimensional array, where each element corresponds to a timestep.\n",
    "        Each element contains the averaged forcing value for the whole catchment\n",
    "        over one timestep.\n",
    "    '''\n",
    "    result = np.zeros(flat_raster.shape[0])\n",
    "    result = np.sum(flat_raster[:, cell_ids] * factors, axis=1)\n",
    "    sum_of_weights = np.sum(factors)\n",
    "    result /= sum_of_weights\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_shared(variable: str,\n",
    "                         times: np.ndarray,\n",
    "                         shm_name: str,\n",
    "                         shape: np.dtype.shape,\n",
    "                         dtype: np.dtype, \n",
    "                         chunk: gpd.GeoDataFrame) -> xr.DataArray:\n",
    "    '''  \n",
    "    Process the gridded forcings chunk loaded into a SharedMemory block. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    variable : str\n",
    "        Name of forcing variable to be processed.\n",
    "    times : np.ndarray\n",
    "        Timesteps in gridded forcings chunk.\n",
    "    shm_name : str\n",
    "        Unique name that identifies the SharedMemory block.\n",
    "    shape : np.dtype.shape\n",
    "        A shape object with dimensions (# timesteps, # of raster cells) in\n",
    "        reference to the gridded forcings chunk.\n",
    "    dtype : np.dtype\n",
    "        Data type of objects in the gridded forcings chunk.\n",
    "    chunk : gpd.GeoDataFrame\n",
    "        A chunk of gridded forcings data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Averaged forcings data for each timestep for each catchment.\n",
    "    '''\n",
    "    existing_shm = shared_memory.SharedMemory(name=shm_name)\n",
    "    raster = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)\n",
    "    results = []\n",
    "\n",
    "    for catchment in chunk.index.unique():\n",
    "        cell_ids = chunk.loc[catchment][\"cell_id\"]\n",
    "        weights = chunk.loc[catchment][\"coverage\"]\n",
    "        mean_at_timesteps = weighted_sum_of_cells(raster, cell_ids, weights)\n",
    "        temp_da = xr.DataArray(\n",
    "            mean_at_timesteps,\n",
    "            dims=[\"time\"],\n",
    "            coords={\"time\": times},\n",
    "            name=f\"{variable}_{catchment}\",\n",
    "        )\n",
    "        temp_da = temp_da.assign_coords(catchment=catchment)\n",
    "        results.append(temp_da)\n",
    "    existing_shm.close()\n",
    "    return xr.concat(results, dim=\"catchment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_APCP_SURFACE_to_dataset(dataset: xr.Dataset) -> xr.Dataset:\n",
    "    '''Convert precipitation value to correct units.'''\n",
    "    # precip_rate is mm/s\n",
    "    # cfe says input atmosphere_water__liquid_equivalent_precipitation_rate is \n",
    "    # mm/h\n",
    "    # nom says prcpnonc input is mm/s\n",
    "    # technically should be kg/m^2/s at 1kg = 1l it equates to mm/s\n",
    "    # nom says qinsur output is m/s, hopefully qinsur is converted to mm/h by \n",
    "    # ngen\n",
    "    dataset[\"APCP_surface\"] = dataset[\"precip_rate\"] * 3600\n",
    "    dataset[\"APCP_surface\"].attrs[\"units\"] = \"mm h^-1\" # ^-1 notation copied \n",
    "                                                        # from source data\n",
    "    dataset[\"APCP_surface\"].attrs[\"source_note\"] = \"This is just the \"\\\n",
    "        \"precip_rate variable converted to mm/h by multiplying by 3600\"\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(forcings_dir: Path,\n",
    "                  variables: dict,\n",
    "                  units: dict) -> None:\n",
    "    '''  \n",
    "    Write outputs to disk in the form of a NetCDF file, using dask clusters to\n",
    "    facilitate parallel computing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    forcings_dir : Path\n",
    "        Path to directory where outputs are to be stored.\n",
    "    variables : dict\n",
    "        Preset dictionary where the keys are forcing variable names and the \n",
    "        values are units.\n",
    "    units : dict\n",
    "        Dictionary where the keys are forcing variable names and the values are \n",
    "        units. Differs from variables, as this dictionary depends on the gridded\n",
    "        forcing dataset.\n",
    "    '''\n",
    "    # start a dask cluster if there isn't one already running\n",
    "    try:\n",
    "        client = Client.current()\n",
    "    except ValueError:\n",
    "        cluster = LocalCluster()\n",
    "        client = Client(cluster)\n",
    "    temp_forcings_dir = forcings_dir / \"temp\"\n",
    "    # Combine all variables into a single dataset using dask\n",
    "    results = [xr.open_dataset(file, chunks=\"auto\") \n",
    "               for file in temp_forcings_dir.glob(\"*.nc\")]\n",
    "    final_ds = xr.merge(results)\n",
    "    for var in final_ds.data_vars:\n",
    "        if var in units:\n",
    "            final_ds[var].attrs[\"units\"] = units[var]\n",
    "        else:\n",
    "            logger.warning(f\"Variable {var} has no units\")\n",
    "\n",
    "    rename_dict = {}\n",
    "    for key, value in variables.items():\n",
    "        if key in final_ds:\n",
    "            rename_dict[key] = value\n",
    "\n",
    "    final_ds = final_ds.rename_vars(rename_dict)\n",
    "    final_ds = add_APCP_SURFACE_to_dataset(final_ds)\n",
    "\n",
    "    # this step halves the storage size of the forcings\n",
    "    for var in final_ds.data_vars:\n",
    "        final_ds[var] = final_ds[var].astype(np.float32)\n",
    "\n",
    "    logger.info(\"Saving to disk\")\n",
    "    # The format for the netcdf is to support a legacy format\n",
    "    # which is why it's a little \"unorthodox\"\n",
    "    # There are no coordinates, just dimensions, catchment ids are stored in a \n",
    "    # 1d data var and time is stored in a 2d data var with the same time array \n",
    "    # for every catchment\n",
    "    # time is stored as unix timestamps, units have to be set\n",
    "\n",
    "    # add the catchment ids as a 1d data var\n",
    "    final_ds[\"ids\"] = final_ds[\"catchment\"].astype(str)\n",
    "    # time needs to be a 2d array of the same time array as unix timestamps for \n",
    "    # every catchment\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        time_array = (\n",
    "            final_ds.time.astype(\"datetime64[s]\").astype(np.int64).values//10**9\n",
    "        )  ## convert from ns to s\n",
    "    time_array = time_array.astype(np.int32) ## convert to int32 to save space\n",
    "    final_ds = final_ds.drop_vars([\"catchment\", \"time\"]) ## drop the original \n",
    "                                                    # time and catchment vars\n",
    "    final_ds = final_ds.rename_dims({\"catchment\": \"catchment-id\"}) # rename the \n",
    "                                                        # catchment dimension\n",
    "    # add the time as a 2d data var, yes this is wasting disk space.\n",
    "    final_ds[\"Time\"] = ((\"catchment-id\", \"time\"), \n",
    "                        [time_array for _ in range(len(final_ds[\"ids\"]))])\n",
    "    # set the time unit\n",
    "    final_ds[\"Time\"].attrs[\"units\"] = \"s\"\n",
    "    final_ds[\"Time\"].attrs[\"epoch_start\"] = \"01/01/1970 00:00:00\" # not needed \n",
    "                                            # but suppresses the ngen warning\n",
    "\n",
    "    final_ds.to_netcdf(forcings_dir / \"forcings.nc\", engine=\"netcdf4\")\n",
    "    # close the datasets\n",
    "    _ = [result.close() for result in results]\n",
    "    final_ds.close()\n",
    "\n",
    "    # clean up the temp files\n",
    "    for file in temp_forcings_dir.glob(\"*.*\"):\n",
    "        file.unlink()\n",
    "    temp_forcings_dir.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zonal_stats(\n",
    "    gdf: gpd.GeoDataFrame, merged_data: xr.Dataset, forcings_dir: Path\n",
    ") -> None:\n",
    "    '''  \n",
    "    Compute zonal statistics in parallel for all timesteps over all desired \n",
    "    catchments. Create chunks of catchments and within those, chunks of \n",
    "    timesteps for memory management.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        Contains identity and geometry information on desired catchments.\n",
    "    merged_data : xr.Dataset\n",
    "        Gridded forcing data that intersects with desired catchments.\n",
    "    forcings_dir : Path\n",
    "        Path to directory where outputs are to be stored.\n",
    "    '''\n",
    "    logger.info(\"Computing zonal stats in parallel for all timesteps\")\n",
    "    timer_start = time.time()\n",
    "    num_partitions = multiprocessing.cpu_count() - 1\n",
    "    if num_partitions > len(gdf):\n",
    "        num_partitions = len(gdf)\n",
    "\n",
    "    catchments = get_cell_weights_parallel(gdf, merged_data, num_partitions)\n",
    "\n",
    "    units = get_units(merged_data)\n",
    "\n",
    "    variables = {\n",
    "                \"LWDOWN\": \"DLWRF_surface\",\n",
    "                \"PSFC\": \"PRES_surface\",\n",
    "                \"Q2D\": \"SPFH_2maboveground\",\n",
    "                \"RAINRATE\": \"precip_rate\",\n",
    "                \"SWDOWN\": \"DSWRF_surface\",\n",
    "                \"T2D\": \"TMP_2maboveground\",\n",
    "                \"U2D\": \"UGRD_10maboveground\",\n",
    "                \"V2D\": \"VGRD_10maboveground\",\n",
    "            }\n",
    "\n",
    "    cat_chunks = np.array_split(catchments, num_partitions)\n",
    "\n",
    "    progress = Progress(\n",
    "        TextColumn(\"[progress.description]{task.description}\"),\n",
    "        BarColumn(),\n",
    "        \"[progress.percentage]{task.percentage:>3.0f}%\",\n",
    "        TextColumn(\"{task.completed}/{task.total}\"),\n",
    "        \"•\",\n",
    "        TextColumn(\" Elapsed Time:\"),\n",
    "        TimeElapsedColumn(),\n",
    "        TextColumn(\" Remaining Time:\"),\n",
    "        TimeRemainingColumn(),\n",
    "    )\n",
    "\n",
    "    timer = time.perf_counter()\n",
    "    variable_task = progress.add_task(\n",
    "        \"[cyan]Processing variables...\", total=len(variables), elapsed=0\n",
    "    )\n",
    "    progress.start()\n",
    "    for variable in variables.keys():\n",
    "        progress.update(variable_task, advance=1)\n",
    "        progress.update(variable_task, description=f\"Processing {variable}\")\n",
    "\n",
    "        if variable not in merged_data.data_vars:\n",
    "            logger.warning(f\"Variable {variable} not in forcings, skipping\")\n",
    "            continue\n",
    "\n",
    "        # to make sure this fits in memory, we need to chunk the data\n",
    "        time_chunks = get_index_chunks(merged_data[variable])\n",
    "        chunk_task = progress.add_task(\"[purple] processing chunks\", \n",
    "                                       total=len(time_chunks))\n",
    "        for i, times in enumerate(time_chunks):\n",
    "            progress.update(chunk_task, advance=1)\n",
    "            start, end = times\n",
    "            # select the chunk of time we want to process\n",
    "            data_chunk = merged_data[variable].isel(time=slice(start,end))\n",
    "            # put it in shared memory\n",
    "            shm, shape, dtype = create_shared_memory(data_chunk)\n",
    "            times = data_chunk.time.values\n",
    "            # create a partial function to pass to the multiprocessing pool\n",
    "            partial_process_chunk = partial(process_chunk_shared,\n",
    "                                            variable,\n",
    "                                            times,\n",
    "                                            shm.name,\n",
    "                                            shape,\n",
    "                                            dtype)\n",
    "\n",
    "            logger.debug(f\"Processing variable: {variable}\")\n",
    "            # process the chunks of catchments in parallel\n",
    "            with multiprocessing.Pool(num_partitions) as pool:\n",
    "                variable_data = pool.map(partial_process_chunk, cat_chunks)\n",
    "            del partial_process_chunk\n",
    "            # clean up the shared memory\n",
    "            shm.close()\n",
    "            shm.unlink()\n",
    "            logger.debug(f\"Processed variable: {variable}\")\n",
    "            concatenated_da = xr.concat(variable_data, dim=\"catchment\")\n",
    "            # delete the data to free up memory\n",
    "            del variable_data\n",
    "            logger.debug(f\"Concatenated variable: {variable}\")\n",
    "            # write this to disk now to save memory\n",
    "            # xarray will monitor memory usage, but it doesn't account for the \n",
    "            # shared memory used to store the raster\n",
    "            # This reduces memory usage by about 60%\n",
    "            concatenated_da.to_dataset(name=variable).to_netcdf(forcings_dir/ \n",
    "                                                                \"temp\" / \n",
    "                                                                f\"{variable}_{i}.nc\")\n",
    "        # Merge the chunks back together\n",
    "        datasets = [xr.open_dataset(forcings_dir \n",
    "                                    / \"temp\" \n",
    "                                    / f\"{variable}_{i}.nc\") \n",
    "                                    for i in range(len(time_chunks))]\n",
    "        result = xr.concat(datasets, dim=\"time\")\n",
    "        result.to_netcdf(forcings_dir / \"temp\" / f\"{variable}.nc\")\n",
    "        # close the datasets\n",
    "        result.close()\n",
    "        _ = [dataset.close() for dataset in datasets]\n",
    "\n",
    "        for file in forcings_dir.glob(\"temp/*_*.nc\"):\n",
    "            file.unlink()\n",
    "        progress.remove_task(chunk_task)\n",
    "    progress.update(\n",
    "        variable_task,\n",
    "        description=f\"Forcings processed in {time.perf_counter() - timer:2f}\"/\n",
    "        \" seconds\",\n",
    "    )\n",
    "    progress.stop()\n",
    "    logger.info(\n",
    "        f\"Forcing generation complete! Zonal stats computed in \"\\\n",
    "            \"{time.time() - timer_start:2f} seconds\"\n",
    "    )\n",
    "    write_outputs(forcings_dir, variables, units)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously open file (unable to open file: name = '/media/volume/Clone_Imp_Data/FORCING/2008/200801010000.LDASIN_DOMAIN1', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'h5netcdf.core.File'>, ('/media/volume/Clone_Imp_Data/FORCING/2008/200801010000.LDASIN_DOMAIN1',), 'r', (('decode_vlen_strings', True), ('driver', None), ('invalid_netcdf', None)), '685c5ed5-8105-46fb-a99f-53c536d10414']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m forc_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/media/volume/Clone_Imp_Data/FORCING/2008/\u001b[39m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m200801010000.LDASIN_DOMAIN1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get CRS from the example NetCDF\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m projection \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforc_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh5netcdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcrs\u001b[38;5;241m.\u001b[39mesri_pe_string\n\u001b[1;32m      8\u001b[0m logging\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot projection from grid file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/api.py:573\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    562\u001b[0m     decode_cf,\n\u001b[1;32m    563\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    570\u001b[0m )\n\u001b[1;32m    572\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 573\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    580\u001b[0m     backend_ds,\n\u001b[1;32m    581\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    592\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:402\u001b[0m, in \u001b[0;36mH5netcdfBackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, format, group, lock, invalid_netcdf, phony_dims, decode_vlen_strings, driver, driver_kwds)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    383\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m     driver_kwds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    400\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m    401\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 402\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mH5NetCDFStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mphony_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mphony_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_vlen_strings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_vlen_strings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdriver_kwds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver_kwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    416\u001b[0m     ds \u001b[38;5;241m=\u001b[39m store_entrypoint\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[1;32m    417\u001b[0m         store,\n\u001b[1;32m    418\u001b[0m         mask_and_scale\u001b[38;5;241m=\u001b[39mmask_and_scale,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    424\u001b[0m         decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m    425\u001b[0m     )\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:182\u001b[0m, in \u001b[0;36mH5NetCDFStore.open\u001b[0;34m(cls, filename, mode, format, group, lock, autoclose, invalid_netcdf, phony_dims, decode_vlen_strings, driver, driver_kwds)\u001b[0m\n\u001b[1;32m    179\u001b[0m         lock \u001b[38;5;241m=\u001b[39m combine_locks([HDF5_LOCK, get_write_lock(filename)])\n\u001b[1;32m    181\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(h5netcdf\u001b[38;5;241m.\u001b[39mFile, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:128\u001b[0m, in \u001b[0;36mH5NetCDFStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# todo: utilizing find_root_and_group seems a bit clunky\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#  making filename available on h5netcdf.Group seems better\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m find_root_and_group(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfilename\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock \u001b[38;5;241m=\u001b[39m ensure_lock(lock)\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:193\u001b[0m, in \u001b[0;36mH5NetCDFStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/h5netcdf_.py:185\u001b[0m, in \u001b[0;36mH5NetCDFStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 185\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_nc4_require_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m            \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_h5netcdf_create_group\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m/usr/lib/python3.12/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/xarray/backends/file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/h5netcdf/core.py:1054\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, path, mode, invalid_netcdf, phony_dims, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preexisting_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(path) \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1053\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h5py \u001b[38;5;241m=\u001b[39m h5py\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h5file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_h5py\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# file-like object\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preexisting_file \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/h5py/_hl/files.py:561\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    552\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    553\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    554\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    555\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    556\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    557\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    558\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    559\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    560\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 561\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m~/nwm_network_lstm_test/.venv/lib/python3.12/site-packages/h5py/_hl/files.py:235\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[1;32m    234\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 235\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    237\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:102\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously open file (unable to open file: name = '/media/volume/Clone_Imp_Data/FORCING/2008/200801010000.LDASIN_DOMAIN1', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Set path to an example FORCINGS NetCDF downloaded from NWM 3.0 retrospective \n",
    "# S3 bucket\n",
    "forc_path = \"/media/volume/Clone_Imp_Data/FORCING/2008/\"\\\n",
    "    \"200801010000.LDASIN_DOMAIN1\"\n",
    "\n",
    "# Get CRS from the example NetCDF\n",
    "projection = xr.open_dataset(forc_path, engine=\"h5netcdf\").crs.esri_pe_string\n",
    "logging.debug(\"Got projection from grid file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hydrofabric geodatabase\n",
    "gdb_path = \"../NWM_v3_hydrofabric.gdb/\"\n",
    "gdb = gpd.read_file(gdb_path, driver=\"FileGDB\", layer=\"nwm_catchments_conus\")\n",
    "start_time = \"2008-01-01 00:00:00\"\n",
    "end_time = \"2008-01-01 23:00:00\"\n",
    "\n",
    "gdb.set_geometry('geometry', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounding box of desired state (in our case, AL)\n",
    "al_bounding_box = {'geometry': [Polygon([\n",
    "    (-88.4745951503515,30.222501133601334),\n",
    "    (-84.89247974539745,35.008322669916694),\n",
    "    (-88.4745951503515,35.008322669916694),\n",
    "    (-84.89247974539745,30.222501133601334)\n",
    "    ])]}\n",
    "al_box = gpd.GeoDataFrame(al_bounding_box, crs=\"EPSG:4326\")\n",
    "al_box.bounds\n",
    "\n",
    "# Transform coordinates to the CRS of the forcing files\n",
    "al_box.to_crs(projection, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip forcing files to bounds of Alabama\n",
    "al_forcs = clip_dataset_to_bounds(\n",
    "    xr.open_mfdataset(\"/media/volume/Clone_Imp_Data/FORCING/2008/\"\\\n",
    "                      \"20080101*.LDASIN_DOMAIN1\"),\n",
    "                      al_box.bounds.loc[0],\n",
    "                       \"2008-01-01 00:00:00\",\n",
    "                       \"2008-01-01 23:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small dictionary for testing.\n",
    "# headwater: tailwater\n",
    "small_dict = {\"445308\": 445314,\n",
    "              \"445322\": 445326,\n",
    "              \"445328\": 445336}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict(studydict: dict,\n",
    "                 gdb: gpd.GeoDataFrame,\n",
    "                 forcs: xr.Dataset,\n",
    "                 start_time: str, \n",
    "                 end_time: str,\n",
    "                 output_file: str) -> None:\n",
    "    '''  \n",
    "    Generate forcing files given a dictionary of desired catchment pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    studydict : dict\n",
    "        Dictionary of desired catchment pairs. The keys are headwaters, and the\n",
    "        values are the corresponding tailwaters.\n",
    "    gdb : gpd.GeoDataFrame\n",
    "        NWM 3.0 retrospective hydrofabric.\n",
    "    forcs : xr.Dataset\n",
    "        Gridded forcings file.\n",
    "    start_time : str\n",
    "        Desired start time in YYYY/MM/DD HH:MM:SS format.\n",
    "    end_time : str\n",
    "        Desired end time in YYYY/MM/DD HH:MM:SS format.\n",
    "    output_file : str\n",
    "        Name of output file to be saved to disk.\n",
    "    '''\n",
    "    for k, v in list(studydict.items()):\n",
    "        head_gdf = head_gdf_selection(k, gdb)\n",
    "        tail_gdf = tail_gdf_selection(v, gdb)\n",
    "\n",
    "        head_forcs = clip_dataset_to_bounds(\n",
    "            forcs, head_gdf.total_bounds, start_time, end_time\n",
    "        )\n",
    "        logging.debug(f\"head gdf bounds: {head_gdf.total_bounds}\")\n",
    "\n",
    "        forcing_working_dir = \"test/\"+k+\"-working-dir\"\n",
    "\n",
    "        if not forcing_working_dir.exists():\n",
    "            forcing_working_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        temp_dir = forcing_working_dir / \"temp\"\n",
    "        if not temp_dir.exists():\n",
    "            temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        compute_zonal_stats(head_gdf, head_forcs, forcing_working_dir)\n",
    "            \n",
    "        shutil.copy(forcing_working_dir / \"forcings.nc\", output_file)\n",
    "        logging.info(f\"Created forcings file: {output_file}\")\n",
    "        # remove the working directory\n",
    "        shutil.rmtree(forcing_working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dict(small_dict, gdb, al_forcs, start_time, end_time, \n",
    "             output_file='test_forcs.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
