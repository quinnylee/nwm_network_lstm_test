{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from typing import Tuple\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import time\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "from exactextract import exact_extract\n",
    "from exactextract.raster import NumPyRasterSource\n",
    "from pathlib import Path\n",
    "from rich.progress import (\n",
    "    Progress,\n",
    "    BarColumn,\n",
    "    TextColumn,\n",
    "    TimeElapsedColumn,\n",
    "    TimeRemainingColumn,\n",
    ")\n",
    "import psutil\n",
    "from math import ceil\n",
    "from multiprocessing import shared_memory\n",
    "from functools import partial\n",
    "import warnings\n",
    "from dask.distributed import Client, LocalCluster\n",
    "import shutil\n",
    "import os\n",
    "from shapely.geometry import Polygon\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    message=\"'DataFrame.swapaxes' is deprecated\", \n",
    "    category=FutureWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \n",
    "    message=\"'GeoDataFrame.swapaxes' is deprecated\", \n",
    "    category=FutureWarning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_time_range(dataset: xr.Dataset, \n",
    "                        start_time: str, \n",
    "                        end_time: str) -> Tuple[str, str]:\n",
    "    '''\n",
    "    Ensure that all selected times are in the passed dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Dataset with a time coordinate.\n",
    "    start_time : str\n",
    "        Desired start time in YYYY/MM/DD HH:MM:SS format.\n",
    "    end_time : str\n",
    "        Desired end time in YYYY/MM/DD HH:MM:SS format.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        start_time, or if not available, earliest available timestep in dataset.\n",
    "    str\n",
    "        end_time, or if not available, latest available timestep in dataset.\n",
    "    '''\n",
    "    end_time_in_dataset = dataset.time.isel(time=-1).values\n",
    "    start_time_in_dataset = dataset.time.isel(time=0).values\n",
    "    if np.datetime64(start_time) < start_time_in_dataset:\n",
    "        logger.warning(\n",
    "            f\"provided start {start_time} is before the start of the dataset \"\\\n",
    "                \"{start_time_in_dataset}, selecting from \"\\\n",
    "                \"{start_time_in_dataset}\"\n",
    "        )\n",
    "        start_time = start_time_in_dataset\n",
    "    if np.datetime64(end_time) > end_time_in_dataset:\n",
    "        logger.warning(\n",
    "            f\"provided end {end_time} is after the end of the dataset \"\\\n",
    "                \"{end_time_in_dataset}, selecting until {end_time_in_dataset}\"\n",
    "        )\n",
    "        end_time = end_time_in_dataset\n",
    "    return start_time, end_time\n",
    "\n",
    "def clip_dataset_to_bounds(dataset: xr.Dataset, \n",
    "                           bounds: Tuple[float, float, float, float], \n",
    "                           start_time: str, \n",
    "                           end_time: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Clip the dataset to specified geographical bounds.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Dataset to be clipped.\n",
    "    bounds : tuple[float, float, float, float]\n",
    "        Corners of bounding box. bounds[0] is x_min, bounds[1] is y_min, \n",
    "        bounds[2] is x_max, bounds[3] is y_max.\n",
    "    start_time : str\n",
    "        Desired start time in YYYY/MM/DD HH:MM:SS format.\n",
    "    end_time : str\n",
    "        Desired end time in YYYY/MM/DD HH:MM:SS format.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xr.Dataset\n",
    "        Clipped dataset.\n",
    "    \"\"\"\n",
    "    # check time range here in case just this function is imported and not the \n",
    "    # whole module\n",
    "    start_time, end_time = validate_time_range(dataset, start_time, end_time)\n",
    "    dataset = dataset.sel(\n",
    "        x=slice(bounds[0], bounds[2]),\n",
    "        y=slice(bounds[1], bounds[3]),\n",
    "        time=slice(start_time, end_time),\n",
    "    )\n",
    "    logger.info(\"Selected time range and clipped to bounds\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_gdf_selection(headwater: str, \n",
    "                       gdb: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    '''  \n",
    "    Select headwater row from GeoDataFrame containing all basins in study area.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    headwater : str\n",
    "        NWM 3.0 reach ID of headwater basin\n",
    "    gdb : gpd.GeoDataFrame\n",
    "        GeoDataFrame that contains geometry information about all basins in\n",
    "        study area.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    head_gdf : gpd.GeoDataFrame\n",
    "        The row in gdb that corresponds to the headwater basin.\n",
    "    '''\n",
    "    head_gdf = gdb.loc[gdb['ID'] == int(headwater)]\n",
    "    return head_gdf\n",
    "\n",
    "def tail_gdf_selection(headwater: str, \n",
    "                       tailwater: str, \n",
    "                       gdb: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    '''  \n",
    "    Select tailwater row from GeoDataFrame containing all basins in study area.\n",
    "    The returned geometry is the union of the polygons for the headwater basin\n",
    "    and the tailwater basin.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    headwater : str\n",
    "        NWM 3.0 reach ID of headwater basin\n",
    "    tailwater : str\n",
    "        NWM 3.0 reach ID of tailwater basin\n",
    "    gdb : gpd.GeoDataFrame\n",
    "        GeoDataFrame that contains geometry information about all basins in\n",
    "        study area.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tail_gdf : gpd.GeoDataFrame\n",
    "        The row in gdb that corresponds to the tailwater basin with a merged\n",
    "        geometry that encompasses both the headwater and tailwater polygons.\n",
    "    '''\n",
    "    tail_geom = gpd.GeoSeries(\n",
    "        [gdb.loc[int(headwater)]['geometry'],\n",
    "        gdb.loc[int(tailwater)]['geometry']]\n",
    "    ).union_all()\n",
    "\n",
    "    d = gdb.loc[gdb['ID'] == int(tailwater)]\n",
    "    d['geometry'] = tail_geom\n",
    "    tail_gdf = gpd.GeoDataFrame(d)\n",
    "\n",
    "    return tail_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_terminals(olddict: dict) -> dict:\n",
    "    '''\n",
    "    Remove entries in olddict whose values are negative numbers, aka the \n",
    "    tailwater basin is a terminal basin (e.g. ocean, lake, reservoir).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    olddict : dict\n",
    "        Dictionary of basin pairs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    newdict : dict\n",
    "        Dictionary of basin pairs that contains no terminal basins as tailwaters\n",
    "    '''\n",
    "    newdict = {k: v for k, v in list(olddict.items()) if v > 0} \n",
    "    return newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_weights(raster: xr.Dataset, \n",
    "                     gdf: gpd.GeoDataFrame, \n",
    "                     wkt: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Get the cell weights (coverage) for each cell in a divide. Coverage is \n",
    "    defined as the fraction (a float in [0,1]) of a raster cell that overlaps \n",
    "    with the polygon in the passed gdf.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raster : xr.Dataset\n",
    "        One timestep of a gridded forcings dataset.\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        A GeoDataFrame with a polygon feature.\n",
    "    wkt : str\n",
    "        Well-known text (WKT) representation of gdf's coordinate reference\n",
    "        system (CRS)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame indexed by divide_id that contains information about coverage\n",
    "        for each raster cell in gridded forcing file.\n",
    "    '''\n",
    "\n",
    "    xmin = raster.x[0]\n",
    "    xmax = raster.x[-1]\n",
    "    ymin = raster.y[0]\n",
    "    ymax = raster.y[-1]\n",
    "    rastersource = NumPyRasterSource(\n",
    "        raster[\"RAINRATE\"], \n",
    "        srs_wkt=wkt, \n",
    "        xmin=xmin, \n",
    "        xmax=xmax, \n",
    "        ymin=ymin, \n",
    "        ymax=ymax\n",
    "    )\n",
    "    output = exact_extract(\n",
    "        rastersource,\n",
    "        gdf,\n",
    "        [\"cell_id\", \"coverage\"],\n",
    "        include_cols=[\"ID\"],\n",
    "        output=\"pandas\",\n",
    "    )\n",
    "    return output.set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_weights_parallel(gdf: gpd.GeoDataFrame,\n",
    "                              input_forcings: xr.Dataset,\n",
    "                              num_partitions: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    Execute get_cell_weights with multiprocessing, with chunking for the passed\n",
    "    GeoDataFrame to conserve memory usage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        A GeoDataFrame with a polygon feature.\n",
    "    input_forcings : xr.Dataset\n",
    "        A gridded forcings file.\n",
    "    num_partitions : int\n",
    "        Number of chunks to split gdf into.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame indexed by divide_id that contains information about coverage\n",
    "        for each raster cell and each timestep in gridded forcing file.\n",
    "    '''\n",
    "    gdf_chunks = np.array_split(gdf, num_partitions)\n",
    "    wkt = gdf.crs.to_wkt()\n",
    "    one_timestep = input_forcings.isel(time=0).compute()\n",
    "    with multiprocessing.Pool() as pool:\n",
    "        args = [(one_timestep, gdf_chunk, wkt) for gdf_chunk in gdf_chunks]\n",
    "        catchments = pool.starmap(get_cell_weights, args)\n",
    "    return pd.concat(catchments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_units(dataset: xr.Dataset) -> dict:\n",
    "    '''\n",
    "    Return dictionary of units for each variable in dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : xr.Dataset\n",
    "        Dataset with variables and units.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict \n",
    "        Each key is a variable in dataset, and its value is the unit associated\n",
    "        with the variable.\n",
    "    '''\n",
    "    units = {}\n",
    "    for var in dataset.data_vars:\n",
    "        if dataset[var].attrs[\"units\"]:\n",
    "            units[var] = dataset[var].attrs[\"units\"]\n",
    "    return units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_chunks(data: xr.DataArray) -> list[tuple[int, int]]:\n",
    "    '''  \n",
    "    Take a DataArray and calculate the start and end index for each chunk based\n",
    "    on the available memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : xr.DataArray\n",
    "        Large DataArray that can't be loaded into memory all at once.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[Tuple[int, int]]\n",
    "        Each element in the list represents a chunk of data. The tuple within\n",
    "        the chunk indicates the start index and end index of the chunk.\n",
    "    '''\n",
    "    array_memory_usage = data.nbytes\n",
    "    free_memory = psutil.virtual_memory().available * 0.8  # 80% of available \n",
    "                                                            # memory\n",
    "    # limit the chunk to 20gb, makes things more stable\n",
    "    free_memory = min(free_memory, 20 * 1024 * 1024 * 1024)\n",
    "    num_chunks = ceil(array_memory_usage / free_memory)\n",
    "    max_index = data.shape[0]\n",
    "    stride = max_index // num_chunks\n",
    "    chunk_start = range(0, max_index, stride)\n",
    "    index_chunks = [(start, start + stride) for start in chunk_start]\n",
    "    return index_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shared_memory(lazy_array: xr.Dataset) -> Tuple[\n",
    "    shared_memory.SharedMemory,\n",
    "    np.dtype.shape,\n",
    "    np.dtype\n",
    "]:\n",
    "    '''\n",
    "    Create a shared memory object so that multiple processes can access loaded \n",
    "    data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lazy_array : xr.Dataset\n",
    "        A chunk of gridded forcing variable data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    shared_memory.SharedMemory\n",
    "        A specific block of memory allocated by the OS of the size of \n",
    "        lazy_array.\n",
    "    np.dtype.shape\n",
    "        A shape object with dimensions (# timesteps, # of raster cells) in\n",
    "        reference to lazy_array.\n",
    "    np.dtype\n",
    "        Data type of objects in lazy_array.\n",
    "    '''\n",
    "    logger.debug(f\"Creating shared memory size {lazy_array.nbytes/ 10**6} Mb.\")\n",
    "    shm = shared_memory.SharedMemory(create=True, size=lazy_array.nbytes)\n",
    "    shared_array = np.ndarray(lazy_array.shape, \n",
    "                              dtype=np.float32, \n",
    "                              buffer=shm.buf)\n",
    "    # if your data is not float32, xarray will do an automatic conversion here\n",
    "    # which consumes a lot more memory, forcings downloaded with this tool will \n",
    "    # work\n",
    "    for start, end in get_index_chunks(lazy_array):\n",
    "        # copy data from lazy to shared memory one chunk at a time\n",
    "        shared_array[start:end] = lazy_array[start:end]\n",
    "\n",
    "    time, x, y = shared_array.shape\n",
    "    shared_array = shared_array.reshape(time, -1)\n",
    "\n",
    "    return shm, shared_array.shape, shared_array.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum_of_cells(flat_raster: np.ndarray, \n",
    "                          cell_ids: np.ndarray, \n",
    "                          factors: np.ndarray) -> np.ndarray:\n",
    "    '''  \n",
    "    Take an average of each forcing variable in a catchment. Create an output\n",
    "    array initialized with zeros. Sum up the forcing variable and divide by the \n",
    "    sum of the cell weights to get an averaged forcing variable for the entire \n",
    "    catchment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    flat_raster : np.ndarray\n",
    "        An array of dimensions (time, x*y) containing forcing variable values\n",
    "        in each cell. Each element in the array corresponds to a cell ID.\n",
    "    cell_ids : np.ndarray\n",
    "        A list of the raster cell IDs that intersect the study catchment.\n",
    "    factors : np.ndarray\n",
    "        A list of the weights (coverages) of each cell in cell_ids.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        An one-dimensional array, where each element corresponds to a timestep.\n",
    "        Each element contains the averaged forcing value for the whole catchment\n",
    "        over one timestep.\n",
    "    '''\n",
    "    result = np.zeros(flat_raster.shape[0])\n",
    "    result = np.sum(flat_raster[:, cell_ids] * factors, axis=1)\n",
    "    sum_of_weights = np.sum(factors)\n",
    "    result /= sum_of_weights\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_shared(variable: str,\n",
    "                         times: np.ndarray,\n",
    "                         shm_name: str,\n",
    "                         shape: np.dtype.shape,\n",
    "                         dtype: np.dtype, \n",
    "                         chunk: gpd.GeoDataFrame) -> xr.DataArray:\n",
    "    '''  \n",
    "    Process the gridded forcings chunk loaded into a SharedMemory block. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    variable : str\n",
    "        Name of forcing variable to be processed.\n",
    "    times : np.ndarray\n",
    "        Timesteps in gridded forcings chunk.\n",
    "    shm_name : str\n",
    "        Unique name that identifies the SharedMemory block.\n",
    "    shape : np.dtype.shape\n",
    "        A shape object with dimensions (# timesteps, # of raster cells) in\n",
    "        reference to the gridded forcings chunk.\n",
    "    dtype : np.dtype\n",
    "        Data type of objects in the gridded forcings chunk.\n",
    "    chunk : gpd.GeoDataFrame\n",
    "        A chunk of gridded forcings data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xr.DataArray\n",
    "        Averaged forcings data for each timestep for each catchment.\n",
    "    '''\n",
    "    existing_shm = shared_memory.SharedMemory(name=shm_name)\n",
    "    raster = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)\n",
    "    results = []\n",
    "\n",
    "    for catchment in chunk.index.unique():\n",
    "        cell_ids = chunk.loc[catchment][\"cell_id\"]\n",
    "        weights = chunk.loc[catchment][\"coverage\"]\n",
    "        mean_at_timesteps = weighted_sum_of_cells(raster, cell_ids, weights)\n",
    "        temp_da = xr.DataArray(\n",
    "            mean_at_timesteps,\n",
    "            dims=[\"time\"],\n",
    "            coords={\"time\": times},\n",
    "            name=f\"{variable}_{catchment}\",\n",
    "        )\n",
    "        temp_da = temp_da.assign_coords(catchment=catchment)\n",
    "        results.append(temp_da)\n",
    "    existing_shm.close()\n",
    "    return xr.concat(results, dim=\"catchment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_APCP_SURFACE_to_dataset(dataset: xr.Dataset) -> xr.Dataset:\n",
    "    '''Convert precipitation value to correct units.'''\n",
    "    # precip_rate is mm/s\n",
    "    # cfe says input atmosphere_water__liquid_equivalent_precipitation_rate is \n",
    "    # mm/h\n",
    "    # nom says prcpnonc input is mm/s\n",
    "    # technically should be kg/m^2/s at 1kg = 1l it equates to mm/s\n",
    "    # nom says qinsur output is m/s, hopefully qinsur is converted to mm/h by \n",
    "    # ngen\n",
    "    dataset[\"APCP_surface\"] = dataset[\"precip_rate\"] * 3600\n",
    "    dataset[\"APCP_surface\"].attrs[\"units\"] = \"mm h^-1\" # ^-1 notation copied \n",
    "                                                        # from source data\n",
    "    dataset[\"APCP_surface\"].attrs[\"source_note\"] = \"This is just the \"\\\n",
    "        \"precip_rate variable converted to mm/h by multiplying by 3600\"\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_outputs(forcings_dir: Path,\n",
    "                  variables: dict,\n",
    "                  units: dict) -> None:\n",
    "    '''  \n",
    "    Write outputs to disk in the form of a NetCDF file, using dask clusters to\n",
    "    facilitate parallel computing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    forcings_dir : Path\n",
    "        Path to directory where outputs are to be stored.\n",
    "    variables : dict\n",
    "        Preset dictionary where the keys are forcing variable names and the \n",
    "        values are units.\n",
    "    units : dict\n",
    "        Dictionary where the keys are forcing variable names and the values are \n",
    "        units. Differs from variables, as this dictionary depends on the gridded\n",
    "        forcing dataset.\n",
    "    '''\n",
    "    # start a dask cluster if there isn't one already running\n",
    "    try:\n",
    "        client = Client.current()\n",
    "    except ValueError:\n",
    "        cluster = LocalCluster()\n",
    "        client = Client(cluster)\n",
    "    temp_forcings_dir = forcings_dir / \"temp\"\n",
    "    # Combine all variables into a single dataset using dask\n",
    "    results = [xr.open_dataset(file, chunks=\"auto\") \n",
    "               for file in temp_forcings_dir.glob(\"*.nc\")]\n",
    "    final_ds = xr.merge(results)\n",
    "    for var in final_ds.data_vars:\n",
    "        if var in units:\n",
    "            final_ds[var].attrs[\"units\"] = units[var]\n",
    "        else:\n",
    "            logger.warning(f\"Variable {var} has no units\")\n",
    "\n",
    "    rename_dict = {}\n",
    "    for key, value in variables.items():\n",
    "        if key in final_ds:\n",
    "            rename_dict[key] = value\n",
    "\n",
    "    final_ds = final_ds.rename_vars(rename_dict)\n",
    "    final_ds = add_APCP_SURFACE_to_dataset(final_ds)\n",
    "\n",
    "    # this step halves the storage size of the forcings\n",
    "    for var in final_ds.data_vars:\n",
    "        final_ds[var] = final_ds[var].astype(np.float32)\n",
    "\n",
    "    logger.info(\"Saving to disk\")\n",
    "    # The format for the netcdf is to support a legacy format\n",
    "    # which is why it's a little \"unorthodox\"\n",
    "    # There are no coordinates, just dimensions, catchment ids are stored in a \n",
    "    # 1d data var and time is stored in a 2d data var with the same time array \n",
    "    # for every catchment\n",
    "    # time is stored as unix timestamps, units have to be set\n",
    "\n",
    "    # add the catchment ids as a 1d data var\n",
    "    final_ds[\"ids\"] = final_ds[\"catchment\"].astype(str)\n",
    "    # time needs to be a 2d array of the same time array as unix timestamps for \n",
    "    # every catchment\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        time_array = (\n",
    "            final_ds.time.astype(\"datetime64[s]\").astype(np.int64).values//10**9\n",
    "        )  ## convert from ns to s\n",
    "    time_array = time_array.astype(np.int32) ## convert to int32 to save space\n",
    "    final_ds = final_ds.drop_vars([\"catchment\", \"time\"]) ## drop the original \n",
    "                                                    # time and catchment vars\n",
    "    final_ds = final_ds.rename_dims({\"catchment\": \"catchment-id\"}) # rename the \n",
    "                                                        # catchment dimension\n",
    "    # add the time as a 2d data var, yes this is wasting disk space.\n",
    "    final_ds[\"Time\"] = ((\"catchment-id\", \"time\"), \n",
    "                        [time_array for _ in range(len(final_ds[\"ids\"]))])\n",
    "    # set the time unit\n",
    "    final_ds[\"Time\"].attrs[\"units\"] = \"s\"\n",
    "    final_ds[\"Time\"].attrs[\"epoch_start\"] = \"01/01/1970 00:00:00\" # not needed \n",
    "                                            # but suppresses the ngen warning\n",
    "\n",
    "    final_ds.to_netcdf(forcings_dir / \"forcings.nc\", engine=\"netcdf4\")\n",
    "    # close the datasets\n",
    "    _ = [result.close() for result in results]\n",
    "    final_ds.close()\n",
    "\n",
    "    # clean up the temp files\n",
    "    for file in temp_forcings_dir.glob(\"*.*\"):\n",
    "        file.unlink()\n",
    "    temp_forcings_dir.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_zonal_stats(\n",
    "    gdf: gpd.GeoDataFrame, merged_data: xr.Dataset, forcings_dir: Path\n",
    ") -> None:\n",
    "    '''  \n",
    "    Compute zonal statistics in parallel for all timesteps over all desired \n",
    "    catchments. Create chunks of catchments and within those, chunks of \n",
    "    timesteps for memory management.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : gpd.GeoDataFrame\n",
    "        Contains identity and geometry information on desired catchments.\n",
    "    merged_data : xr.Dataset\n",
    "        Gridded forcing data that intersects with desired catchments.\n",
    "    forcings_dir : Path\n",
    "        Path to directory where outputs are to be stored.\n",
    "    '''\n",
    "    logger.info(\"Computing zonal stats in parallel for all timesteps\")\n",
    "    timer_start = time.time()\n",
    "    num_partitions = multiprocessing.cpu_count() - 1\n",
    "    if num_partitions > len(gdf):\n",
    "        num_partitions = len(gdf)\n",
    "\n",
    "    catchments = get_cell_weights_parallel(gdf, merged_data, num_partitions)\n",
    "\n",
    "    units = get_units(merged_data)\n",
    "\n",
    "    variables = {\n",
    "                \"LWDOWN\": \"DLWRF_surface\",\n",
    "                \"PSFC\": \"PRES_surface\",\n",
    "                \"Q2D\": \"SPFH_2maboveground\",\n",
    "                \"RAINRATE\": \"precip_rate\",\n",
    "                \"SWDOWN\": \"DSWRF_surface\",\n",
    "                \"T2D\": \"TMP_2maboveground\",\n",
    "                \"U2D\": \"UGRD_10maboveground\",\n",
    "                \"V2D\": \"VGRD_10maboveground\",\n",
    "            }\n",
    "\n",
    "    cat_chunks = np.array_split(catchments, num_partitions)\n",
    "\n",
    "    progress = Progress(\n",
    "        TextColumn(\"[progress.description]{task.description}\"),\n",
    "        BarColumn(),\n",
    "        \"[progress.percentage]{task.percentage:>3.0f}%\",\n",
    "        TextColumn(\"{task.completed}/{task.total}\"),\n",
    "        \"•\",\n",
    "        TextColumn(\" Elapsed Time:\"),\n",
    "        TimeElapsedColumn(),\n",
    "        TextColumn(\" Remaining Time:\"),\n",
    "        TimeRemainingColumn(),\n",
    "    )\n",
    "\n",
    "    timer = time.perf_counter()\n",
    "    variable_task = progress.add_task(\n",
    "        \"[cyan]Processing variables...\", total=len(variables), elapsed=0\n",
    "    )\n",
    "    progress.start()\n",
    "    for variable in variables.keys():\n",
    "        progress.update(variable_task, advance=1)\n",
    "        progress.update(variable_task, description=f\"Processing {variable}\")\n",
    "\n",
    "        if variable not in merged_data.data_vars:\n",
    "            logger.warning(f\"Variable {variable} not in forcings, skipping\")\n",
    "            continue\n",
    "\n",
    "        # to make sure this fits in memory, we need to chunk the data\n",
    "        time_chunks = get_index_chunks(merged_data[variable])\n",
    "        chunk_task = progress.add_task(\"[purple] processing chunks\", \n",
    "                                       total=len(time_chunks))\n",
    "        for i, times in enumerate(time_chunks):\n",
    "            progress.update(chunk_task, advance=1)\n",
    "            start, end = times\n",
    "            # select the chunk of time we want to process\n",
    "            data_chunk = merged_data[variable].isel(time=slice(start,end))\n",
    "            # put it in shared memory\n",
    "            shm, shape, dtype = create_shared_memory(data_chunk)\n",
    "            times = data_chunk.time.values\n",
    "            # create a partial function to pass to the multiprocessing pool\n",
    "            partial_process_chunk = partial(process_chunk_shared,\n",
    "                                            variable,\n",
    "                                            times,\n",
    "                                            shm.name,\n",
    "                                            shape,\n",
    "                                            dtype)\n",
    "\n",
    "            logger.debug(f\"Processing variable: {variable}\")\n",
    "            # process the chunks of catchments in parallel\n",
    "            with multiprocessing.Pool(num_partitions) as pool:\n",
    "                variable_data = pool.map(partial_process_chunk, cat_chunks)\n",
    "            del partial_process_chunk\n",
    "            # clean up the shared memory\n",
    "            shm.close()\n",
    "            shm.unlink()\n",
    "            logger.debug(f\"Processed variable: {variable}\")\n",
    "            concatenated_da = xr.concat(variable_data, dim=\"catchment\")\n",
    "            # delete the data to free up memory\n",
    "            del variable_data\n",
    "            logger.debug(f\"Concatenated variable: {variable}\")\n",
    "            # write this to disk now to save memory\n",
    "            # xarray will monitor memory usage, but it doesn't account for the \n",
    "            # shared memory used to store the raster\n",
    "            # This reduces memory usage by about 60%\n",
    "            concatenated_da.to_dataset(name=variable).to_netcdf(forcings_dir/ \n",
    "                                                                \"temp\" / \n",
    "                                                                f\"{variable}_{i}.nc\")\n",
    "        # Merge the chunks back together\n",
    "        datasets = [xr.open_dataset(forcings_dir \n",
    "                                    / \"temp\" \n",
    "                                    / f\"{variable}_{i}.nc\") \n",
    "                                    for i in range(len(time_chunks))]\n",
    "        result = xr.concat(datasets, dim=\"time\")\n",
    "        result.to_netcdf(forcings_dir / \"temp\" / f\"{variable}.nc\")\n",
    "        # close the datasets\n",
    "        result.close()\n",
    "        _ = [dataset.close() for dataset in datasets]\n",
    "\n",
    "        for file in forcings_dir.glob(\"temp/*_*.nc\"):\n",
    "            file.unlink()\n",
    "        progress.remove_task(chunk_task)\n",
    "    progress.update(\n",
    "        variable_task,\n",
    "        description=f\"Forcings processed in {time.perf_counter() - timer:2f}\"/\n",
    "        \" seconds\",\n",
    "    )\n",
    "    progress.stop()\n",
    "    logger.info(\n",
    "        f\"Forcing generation complete! Zonal stats computed in \"\\\n",
    "            \"{time.time() - timer_start:2f} seconds\"\n",
    "    )\n",
    "    write_outputs(forcings_dir, variables, units)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to an example FORCINGS NetCDF downloaded from NWM 3.0 retrospective \n",
    "# S3 bucket\n",
    "forc_path = \"/media/volume/Clone_Imp_Data/FORCING/2008/\"\\\n",
    "    \"200801010000.LDASIN_DOMAIN1\"\n",
    "\n",
    "# Get CRS from the example NetCDF\n",
    "projection = xr.open_dataset(forc_path, engine=\"h5netcdf\").crs.esri_pe_string\n",
    "logging.debug(\"Got projection from grid file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/nwm_network_lstm_test/.venv/lib/python3.10/site-packages/pyogrio/raw.py:198: RuntimeWarning: driver OpenFileGDB does not support open option DRIVER\n",
      "  return ogr_read(\n",
      "/home/exouser/nwm_network_lstm_test/.venv/lib/python3.10/site-packages/pyogrio/raw.py:198: RuntimeWarning: organizePolygons() received a polygon with more than 100 parts.  The processing may be really slow.  You can skip the processing by setting METHOD=SKIP.\n",
      "  return ogr_read(\n"
     ]
    }
   ],
   "source": [
    "# Read hydrofabric geodatabase\n",
    "gdb_path = \"../NWM_v3_hydrofabric.gdb/\"\n",
    "gdb = gpd.read_file(gdb_path, driver=\"FileGDB\", layer=\"nwm_catchments_conus\")\n",
    "start_time = \"2008-01-01 00:00:00\"\n",
    "end_time = \"2008-01-01 23:00:00\"\n",
    "\n",
    "gdb.set_geometry('geometry', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bounding box of desired state (in our case, AL)\n",
    "al_bounding_box = {'geometry': [Polygon([\n",
    "    (-88.4745951503515,30.222501133601334),\n",
    "    (-84.89247974539745,35.008322669916694),\n",
    "    (-88.4745951503515,35.008322669916694),\n",
    "    (-84.89247974539745,30.222501133601334)\n",
    "    ])]}\n",
    "al_box = gpd.GeoDataFrame(al_bounding_box, crs=\"EPSG:4326\")\n",
    "al_box.bounds\n",
    "\n",
    "# Transform coordinates to the CRS of the forcing files\n",
    "al_box.to_crs(projection, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minx    7.606986e+05\n",
      "miny   -1.024481e+06\n",
      "maxx    1.157581e+06\n",
      "maxy   -4.596480e+05\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(al_box.bounds.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4104/1096619619.py:68: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  x=slice(bounds[0], bounds[2]),\n",
      "/tmp/ipykernel_4104/1096619619.py:69: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y=slice(bounds[1], bounds[3]),\n"
     ]
    }
   ],
   "source": [
    "# Clip forcing files to bounds of Alabama\n",
    "al_forcs = clip_dataset_to_bounds(\n",
    "    xr.open_mfdataset(\"/media/volume/Clone_Imp_Data/FORCING/2008/\"\\\n",
    "                      \"20080101*.LDASIN_DOMAIN1\"),\n",
    "                      al_box.bounds.loc[0],\n",
    "                       \"2008-01-01 00:00:00\",\n",
    "                       \"2008-01-01 23:00:00\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A small dictionary for testing.\n",
    "# headwater: tailwater\n",
    "small_dict = {\"445308\": 445314,\n",
    "              \"445322\": 445326,\n",
    "              \"445328\": 445336}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict(studydict: dict,\n",
    "                 gdb: gpd.GeoDataFrame,\n",
    "                 forcs: xr.Dataset,\n",
    "                 start_time: str, \n",
    "                 end_time: str,\n",
    "                 output_file: str) -> None:\n",
    "    '''  \n",
    "    Generate forcing files given a dictionary of desired catchment pairs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    studydict : dict\n",
    "        Dictionary of desired catchment pairs. The keys are headwaters, and the\n",
    "        values are the corresponding tailwaters.\n",
    "    gdb : gpd.GeoDataFrame\n",
    "        NWM 3.0 retrospective hydrofabric.\n",
    "    forcs : xr.Dataset\n",
    "        Gridded forcings file.\n",
    "    start_time : str\n",
    "        Desired start time in YYYY/MM/DD HH:MM:SS format.\n",
    "    end_time : str\n",
    "        Desired end time in YYYY/MM/DD HH:MM:SS format.\n",
    "    output_file : str\n",
    "        Name of output file to be saved to disk.\n",
    "    '''\n",
    "    for k, v in list(studydict.items()):\n",
    "        head_gdf = head_gdf_selection(k, gdb).to_crs(projection)\n",
    "        print(head_gdf.head())\n",
    "        tail_gdf = tail_gdf_selection(k, v, gdb)\n",
    "\n",
    "        head_forcs = clip_dataset_to_bounds(\n",
    "            forcs, head_gdf.total_bounds, start_time, end_time\n",
    "        )\n",
    "        logging.debug(f\"head gdf bounds: {head_gdf.total_bounds}\")\n",
    "\n",
    "        forcing_working_dir = Path(f\"test/{k}-working-dir\")\n",
    "\n",
    "        if not forcing_working_dir.exists():\n",
    "            forcing_working_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        temp_dir = forcing_working_dir / \"temp\"\n",
    "        if not temp_dir.exists():\n",
    "            temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        compute_zonal_stats(head_gdf, head_forcs, forcing_working_dir)\n",
    "            \n",
    "        shutil.copy(forcing_working_dir / \"forcings.nc\", output_file)\n",
    "        logging.info(f\"Created forcings file: {output_file}\")\n",
    "        # remove the working directory\n",
    "        shutil.rmtree(forcing_working_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ID  Shape_Length  Shape_Area  \\\n",
      "187463  445308       0.13726    0.000558   \n",
      "\n",
      "                                                 geometry  \n",
      "187463  MULTIPOLYGON (((906665.073 -874595.239, 906694...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exouser/nwm_network_lstm_test/.venv/lib/python3.10/site-packages/geopandas/geodataframe.py:1819: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocess_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmall_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mal_forcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest_forcs.nc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 45\u001b[0m, in \u001b[0;36mprocess_dict\u001b[0;34m(studydict, gdb, forcs, start_time, end_time, output_file)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m temp_dir\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     43\u001b[0m     temp_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mcompute_zonal_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_gdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_forcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforcing_working_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m shutil\u001b[38;5;241m.\u001b[39mcopy(forcing_working_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforcings.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m, output_file)\n\u001b[1;32m     48\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated forcings file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[17], line 26\u001b[0m, in \u001b[0;36mcompute_zonal_stats\u001b[0;34m(gdf, merged_data, forcings_dir)\u001b[0m\n\u001b[1;32m     22\u001b[0m     num_partitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gdf)\n\u001b[1;32m     24\u001b[0m catchments \u001b[38;5;241m=\u001b[39m get_cell_weights_parallel(gdf, merged_data, num_partitions)\n\u001b[0;32m---> 26\u001b[0m units \u001b[38;5;241m=\u001b[39m \u001b[43mget_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m variables \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLWDOWN\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDLWRF_surface\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPSFC\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRES_surface\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV2D\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGRD_10maboveground\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m         }\n\u001b[1;32m     39\u001b[0m cat_chunks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray_split(catchments, num_partitions)\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mget_units\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     16\u001b[0m units \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mdata_vars:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munits\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[1;32m     19\u001b[0m         units[var] \u001b[38;5;241m=\u001b[39m dataset[var]\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m units\n",
      "\u001b[0;31mKeyError\u001b[0m: 'units'"
     ]
    }
   ],
   "source": [
    "process_dict(small_dict, gdb, al_forcs, start_time, end_time, \n",
    "             output_file='test_forcs.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
